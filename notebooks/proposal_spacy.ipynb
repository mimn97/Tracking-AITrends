{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTthWTiv2j8",
        "outputId": "fbeef237-e368-45ba-9ed5-ac0969dcddf5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import docx2txt\n",
        "import glob\n",
        "import re \n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords = en.Defaults.stop_words\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqFhqeCXv2kD"
      },
      "source": [
        "# Pre-process dataset & SpaCy's NER Tagging"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HA6vSpI4v2kG"
      },
      "source": [
        "`display_ents` can display the named entities from the entire text. Put the entire path of the file you want to see. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oPjXlgYMv2kH"
      },
      "outputs": [],
      "source": [
        "def display_ents(raw):\n",
        "    doc = en(raw)\n",
        "    displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWmK5F-nBYWg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vGcAkiuEv2kI"
      },
      "outputs": [],
      "source": [
        "def parse_doc(file_path):\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        doc = docx2txt.process(f)\n",
        "\n",
        "    word_par = [word for word in doc.rstrip().split('\\n\\n') if word.lower() not in stopwords] # paragraph-level \n",
        "\n",
        "    new_text_par = \" \".join(word_par)\n",
        "    new_text_par = re.sub('\\s+', ' ', new_text_par)\n",
        "\n",
        "    new_text_sent = tokenize.sent_tokenize(new_text_par)\n",
        "\n",
        "    return new_text_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mgIiAzFC8udS"
      },
      "outputs": [],
      "source": [
        "def show_docx_ents(sponsor_directory):\n",
        "\n",
        "    # Prepare a pandas dataframe\n",
        "    ent_df = pd.DataFrame(columns=['entity_type', 'keyword', 'frequency'])\n",
        "    ent_dict = defaultdict(lambda: defaultdict(int))\n",
        "    ent_type_lst = []\n",
        "    ent_keyword_lst = []\n",
        "    ent_freq_lst = []\n",
        "    \n",
        "    # for each file in the directory\n",
        "    all_files = glob.glob(sponsor_directory + '\\**\\*.docx', recursive = True) # needs correction to be '/' if needed (Windows)\n",
        "    for file in all_files:\n",
        "        # parse docx in python-readable format\n",
        "        parsed_txt = parse_doc(file)\n",
        "\n",
        "        # apply a pre-trained spaCy model to the raw text\n",
        "        for sent in parsed_txt:\n",
        "          ent_sent = en(sent)\n",
        "\n",
        "          # Count the frequency of keywords under each entity type and entity word\n",
        "          if ent_sent.ents:\n",
        "            for ent_word in ent_sent.ents:\n",
        "              ent_dict[ent_word.label_][ent_word.text] += 1\n",
        "              # print(ent_word.text+' - ' +str(ent_word.start_char) +' - '+ str(ent_word.end_char) +' - '+ent_word.label_+ ' - '+str(spacy.explain(ent_word.label_)))\n",
        "          else:\n",
        "            print('No named entities found.')\n",
        "\n",
        "    # dictionary to pandas dataframe\n",
        "    for each_ent in ent_dict.keys():\n",
        "        each_ent_info = ent_dict[each_ent]\n",
        "\n",
        "        for ent_keyword in each_ent_info.keys():\n",
        "            ent_type_lst.append(each_ent)\n",
        "            ent_keyword_lst.append(ent_keyword)\n",
        "            ent_freq_lst.append(each_ent_info[ent_keyword])\n",
        "      \n",
        "    # Insert rows to dataframe\n",
        "    ent_df['entity_type'] = ent_type_lst\n",
        "    ent_df['keyword'] = ent_keyword_lst\n",
        "    ent_df['frequency'] = ent_freq_lst\n",
        "\n",
        "    # save to csv\n",
        "    ent_df.to_csv(sponsor_directory + '\\entity_info.csv', index=True)\n",
        "    sponsor_team = sponsor_directory.split('\\\\')[-1]\n",
        "\n",
        "    print(f'done saving csv with entity info for {sponsor_team}!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "FX__QcLNEx6J"
      },
      "outputs": [],
      "source": [
        "def show_and_filter_ents_all(folder_directory, year, h): # input: year (Fiscal year), h (H1 or H2)\n",
        "\n",
        "    # Concatenate all entity infos from each sponsor team's 'entity_info.csv'\n",
        "\n",
        "    sponsors_dir = glob.glob(folder_directory + '\\*\\entity_info.csv', recursive=True)\n",
        "\n",
        "    df = pd.read_csv(sponsors_dir[0])\n",
        "    df['team'] = sponsors_dir[0].split('\\\\')[-2]\n",
        "\n",
        "    for sponsor_file in sponsors_dir[1:]:\n",
        "      new = pd.read_csv(sponsor_file)\n",
        "      new['team'] = sponsor_file.split('\\\\')[-2]\n",
        "\n",
        "      df = pd.concat([df, new], ignore_index=True)\n",
        "\n",
        "    df['year'] = year\n",
        "    df['H'] = h\n",
        "\n",
        "    # Uncomment if you want to save the merged entity info all across projects at year and H to csv file!\n",
        "    # df.to_csv(folder_directory + f'\\{year}_{h}_entity.csv', index=False)\n",
        "\n",
        "    # Filter out non-related entities\n",
        "    \n",
        "    df_cleaned = df.copy()\n",
        "    \n",
        "    # (1) non-related entity types (based on spaCy)\n",
        "    non_related_entity = ['LOC', 'FAC', 'QUANTITY', 'GPE', 'PERSON', 'CARDINAL', 'TIME', \n",
        "                          'DATE', 'NORP', 'MONEY', 'ORDINAL', 'PERCENT'] # add more if you find new ones\n",
        "    df_cleaned = df_cleaned.loc[~df_cleaned.entity_type.isin(non_related_entity)]\n",
        "\n",
        "    # (2) only AI-related keywords\n",
        "    with open('data\\ai_related_words.txt', 'r') as f:\n",
        "      ai_related_word = [line.rstrip().lower() for line in f]\n",
        "\n",
        "    joined_ai_related_word = '|'.join(ai_related_word)\n",
        "    df_cleaned = df_cleaned.loc[df_cleaned.keyword.str.lower().str.contains(joined_ai_related_word)]\n",
        "    \n",
        "    # Save the cleaned dataframe to csv format\n",
        "    df_cleaned = df_cleaned.reset_index(drop=True)\n",
        "    df_cleaned.to_csv(folder_directory + f'\\cleaned_{year}_{h}_entity.csv', index=False)\n",
        "\n",
        "    return df_cleaned\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bbe9b149b8e435a280e9e56be5b845d59cf4107a9b9b9e530e3e2d9525d477bb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
